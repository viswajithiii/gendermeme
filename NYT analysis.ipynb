{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "from analysis.gender import gender, gender_special\n",
    "from analysis.utils import get_people_mentioned, get_gender, get_quotes, get_associated_verbs, identify_sources, get_associated_adjectives\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_nyt_data(year, month, folder='annotated/NYT/'):\n",
    "    nyt_data = {}\n",
    "    with open('{}nyt_annotated_{}_{}.tsv'.format(folder, year, month), 'r') as nyt_f:\n",
    "        for line in nyt_f:\n",
    "            link, data, corenlp = line.strip().split('\\t')\n",
    "            nyt_data[link] = {'data': json.loads(data), 'corenlp': json.loads(corenlp)}\n",
    "    \n",
    "    return nyt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mentions_quotes(nyt_data, out_fn):\n",
    "    for link, values in nyt_data.iteritems():\n",
    "        data = values['data']\n",
    "        corenlp = values['corenlp']\n",
    "        if not type(corenlp) is dict: # This happens when CoreNLP timed out\n",
    "            continue\n",
    "        corefs = corenlp['corefs']\n",
    "        sentences = corenlp['sentences']\n",
    "        pm = get_people_mentioned(sentences, corefs, include_gender=True)\n",
    "        quotes = get_quotes(pm, sentences, corefs)\n",
    "        num_mentions = {'MALE': 0, 'FEMALE': 0}\n",
    "        num_distinct_mentions = {'MALE': 0, 'FEMALE': 0}\n",
    "        num_quoted_words = {'MALE': 0, 'FEMALE': 0}\n",
    "        num_quoted_people = {'MALE': 0, 'FEMALE': 0}\n",
    "        for person, info in pm.iteritems():\n",
    "            count = info[0]\n",
    "            gender = info[1][0]\n",
    "            if not type(gender) is str:\n",
    "                continue\n",
    "            if 'ambiguous' in gender.lower():  # For now, ignore ambiguous cases\n",
    "                continue\n",
    "            num_mentions[gender.upper()] += count\n",
    "            num_distinct_mentions[gender.upper()] += 1\n",
    "            if person in quotes:\n",
    "                num_quoted_people[gender.upper()] += 1\n",
    "                quote_length = len(quotes[person])\n",
    "                num_quoted_words[gender.upper()] += quote_length\n",
    "            \n",
    "\n",
    "        author_gender = 'UNKNOWN'\n",
    "        if 'print_byline' in data:\n",
    "            pb = data['print_byline']\n",
    "            if pb.startswith('By'):\n",
    "                pb = pb[3:]\n",
    "            if len(pb) > 0:\n",
    "                author_gender = get_gender(pb)\n",
    "        elif 'norm_byline' in data:\n",
    "            author_gender = get_gender(data['norm_byline'][data['norm_byline'].find(',') + 1:])\n",
    "        if not type(author_gender) is str:\n",
    "            author_gender = 'UNKNOWN'\n",
    "        else:\n",
    "            author_gender = author_gender.upper()\n",
    "        '''\n",
    "        print author_gender\n",
    "        print data['id']\n",
    "        print data.keys()\n",
    "        print data['print_byline']\n",
    "        print data['norm_byline']\n",
    "        '''\n",
    "        year, month = data['id'].split('_')[:2]\n",
    "        with open(out_fn, 'a') as out_f:\n",
    "            try:\n",
    "                out_f.write('\\t'.join([unicode(a) for a in [\n",
    "                            link, author_gender, year, month, data.get('section', ''), \n",
    "                            ','.join([unicode(d) for d in data.get('descriptors', [])]), \n",
    "                            num_distinct_mentions['MALE'], num_distinct_mentions['FEMALE'],\n",
    "                            num_mentions['MALE'], num_mentions['FEMALE'],\n",
    "                            num_quoted_people['MALE'], num_quoted_people['FEMALE'],\n",
    "                            num_quoted_words['MALE'], num_quoted_words['FEMALE'],\n",
    "                            ]]))\n",
    "                out_f.write('\\n')\n",
    "            except UnicodeEncodeError:\n",
    "                pass\n",
    "            except:\n",
    "                print link\n",
    "                print author_gender\n",
    "                print year\n",
    "                print month\n",
    "                print data.get('section', '')\n",
    "                print data.get('descriptors')\n",
    "                print ','.join(data.get('descriptors', []))\n",
    "                print num_distinct_mentions\n",
    "                print num_mentions\n",
    "                print num_quotes\n",
    "                print [\n",
    "                            link, author_gender, year, month, data.get('section', ''), ','.join(data.get('descriptors', [])), \n",
    "                            num_distinct_mentions['MALE'], num_distinct_mentions['FEMALE'],\n",
    "                            num_mentions['MALE'], num_mentions['FEMALE'],\n",
    "                            num_quotes['MALE'], num_quotes['FEMALE']\n",
    "                            ]\n",
    "                return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nyt_data = load_nyt_data(1989, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_mentions_quotes(nyt_data, 'nyt_data_counts.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb 21 16:00:33 2017 Loading data for 1987/1\n",
      "Tue Feb 21 16:01:07 2017 Analyzing data ...\n",
      "Tue Feb 21 16:01:14 2017 Loading data for 1987/2\n",
      "Tue Feb 21 16:02:14 2017 Analyzing data ...\n",
      "Tue Feb 21 16:02:22 2017 Loading data for 1987/3\n",
      "Tue Feb 21 16:03:12 2017 Analyzing data ...\n",
      "Tue Feb 21 16:03:20 2017 Loading data for 1987/4\n",
      "Tue Feb 21 16:04:14 2017 Analyzing data ...\n",
      "Tue Feb 21 16:04:22 2017 Loading data for 1987/5\n",
      "Tue Feb 21 16:05:11 2017 Analyzing data ...\n",
      "Tue Feb 21 16:05:18 2017 Loading data for 1987/6\n",
      "Tue Feb 21 16:06:00 2017 Analyzing data ...\n",
      "Tue Feb 21 16:06:06 2017 Loading data for 1987/7\n",
      "Tue Feb 21 16:07:03 2017 Analyzing data ...\n",
      "Tue Feb 21 16:07:11 2017 Loading data for 1987/8\n",
      "Tue Feb 21 16:08:05 2017 Analyzing data ...\n",
      "Tue Feb 21 16:08:13 2017 Loading data for 1987/9\n",
      "Tue Feb 21 16:08:56 2017 Analyzing data ...\n",
      "Tue Feb 21 16:09:02 2017 Loading data for 1987/10\n",
      "Tue Feb 21 16:10:06 2017 Analyzing data ...\n",
      "Tue Feb 21 16:10:13 2017 Loading data for 1987/11\n",
      "Tue Feb 21 16:11:50 2017 Analyzing data ...\n",
      "Tue Feb 21 16:12:00 2017 Loading data for 1987/12\n",
      "Tue Feb 21 16:13:02 2017 Analyzing data ...\n",
      "Tue Feb 21 16:13:12 2017 Loading data for 1988/1\n",
      "Tue Feb 21 16:14:09 2017 Analyzing data ...\n",
      "Tue Feb 21 16:14:17 2017 Loading data for 1988/2\n",
      "Tue Feb 21 16:15:01 2017 Analyzing data ...\n",
      "Tue Feb 21 16:15:07 2017 Loading data for 1988/3\n",
      "Tue Feb 21 16:15:53 2017 Analyzing data ...\n",
      "Tue Feb 21 16:16:00 2017 Loading data for 1988/4\n",
      "Tue Feb 21 16:16:42 2017 Analyzing data ...\n",
      "Tue Feb 21 16:16:48 2017 Loading data for 1988/5\n",
      "Tue Feb 21 16:17:33 2017 Analyzing data ...\n",
      "Tue Feb 21 16:17:41 2017 Loading data for 1988/6\n",
      "Tue Feb 21 16:18:25 2017 Analyzing data ...\n",
      "Tue Feb 21 16:18:31 2017 Loading data for 1988/7\n",
      "Tue Feb 21 16:19:17 2017 Analyzing data ...\n",
      "Tue Feb 21 16:19:23 2017 Loading data for 1988/8\n",
      "Tue Feb 21 16:20:13 2017 Analyzing data ...\n",
      "Tue Feb 21 16:20:20 2017 Loading data for 1988/9\n",
      "Tue Feb 21 16:21:03 2017 Analyzing data ...\n",
      "Tue Feb 21 16:21:09 2017 Loading data for 1988/10\n",
      "Tue Feb 21 16:21:51 2017 Analyzing data ...\n",
      "Tue Feb 21 16:21:57 2017 Loading data for 1988/11\n",
      "Tue Feb 21 16:22:48 2017 Analyzing data ...\n",
      "Tue Feb 21 16:22:55 2017 Loading data for 1988/12\n",
      "Tue Feb 21 16:23:47 2017 Analyzing data ...\n",
      "Tue Feb 21 16:23:53 2017 Loading data for 1989/1\n",
      "Tue Feb 21 16:24:38 2017 Analyzing data ...\n",
      "Tue Feb 21 16:24:44 2017 Loading data for 1989/2\n",
      "Tue Feb 21 16:25:29 2017 Analyzing data ...\n",
      "Tue Feb 21 16:25:36 2017 Loading data for 1989/3\n",
      "Tue Feb 21 16:26:16 2017 Analyzing data ...\n",
      "Tue Feb 21 16:26:21 2017 Loading data for 1989/4\n",
      "Tue Feb 21 16:29:56 2017 Analyzing data ...\n",
      "Tue Feb 21 16:30:06 2017 Loading data for 1989/5\n",
      "Tue Feb 21 16:42:15 2017 Analyzing data ...\n",
      "Tue Feb 21 17:19:51 2017 Loading data for 1989/6\n",
      "Tue Feb 21 18:32:22 2017 Analyzing data ...\n",
      "Tue Feb 21 18:32:29 2017 Loading data for 1989/7\n",
      "Tue Feb 21 18:40:26 2017 Analyzing data ...\n",
      "Tue Feb 21 18:40:34 2017 Loading data for 1989/8\n",
      "Tue Feb 21 18:42:24 2017 Analyzing data ...\n",
      "Tue Feb 21 18:42:59 2017 Loading data for 1989/9\n",
      "Tue Feb 21 18:44:18 2017 Analyzing data ...\n",
      "Tue Feb 21 18:57:36 2017 Loading data for 1989/10\n",
      "Tue Feb 21 20:05:43 2017 Analyzing data ...\n",
      "Tue Feb 21 20:43:10 2017 Loading data for 1989/11\n",
      "Tue Feb 21 20:51:06 2017 Analyzing data ...\n",
      "Tue Feb 21 20:51:12 2017 Loading data for 1989/12\n",
      "Tue Feb 21 20:55:12 2017 Analyzing data ...\n",
      "Tue Feb 21 20:55:27 2017 Loading data for 1990/1\n",
      "Tue Feb 21 21:03:55 2017 Analyzing data ...\n",
      "Tue Feb 21 21:04:03 2017 Loading data for 1990/2\n",
      "Tue Feb 21 21:10:52 2017 Analyzing data ...\n",
      "Tue Feb 21 21:11:00 2017 Loading data for 1990/3\n",
      "Tue Feb 21 21:25:35 2017 Analyzing data ...\n",
      "Tue Feb 21 21:25:41 2017 Loading data for 1990/4\n",
      "Tue Feb 21 21:31:44 2017 Analyzing data ...\n",
      "Tue Feb 21 21:31:51 2017 Loading data for 1990/5\n",
      "Tue Feb 21 21:43:24 2017 Analyzing data ...\n",
      "Tue Feb 21 21:43:31 2017 Loading data for 1990/6\n",
      "Tue Feb 21 21:59:10 2017 Analyzing data ...\n",
      "Tue Feb 21 21:59:17 2017 Loading data for 1990/7\n",
      "Tue Feb 21 22:05:27 2017 Analyzing data ...\n",
      "Tue Feb 21 22:06:23 2017 Loading data for 1990/8\n",
      "Tue Feb 21 22:08:14 2017 Analyzing data ...\n",
      "Tue Feb 21 22:08:21 2017 Loading data for 1990/9\n",
      "Tue Feb 21 22:17:23 2017 Analyzing data ...\n",
      "Tue Feb 21 22:17:28 2017 Loading data for 1990/10\n",
      "Tue Feb 21 22:19:26 2017 Analyzing data ...\n",
      "Tue Feb 21 22:19:35 2017 Loading data for 1990/11\n",
      "Tue Feb 21 22:20:50 2017 Analyzing data ...\n",
      "Tue Feb 21 22:20:58 2017 Loading data for 1990/12\n",
      "Tue Feb 21 22:24:08 2017 Analyzing data ...\n",
      "Tue Feb 21 22:24:15 2017 Loading data for 1991/1\n",
      "Tue Feb 21 22:26:43 2017 Analyzing data ...\n",
      "Tue Feb 21 22:26:48 2017 Loading data for 1991/2\n",
      "Tue Feb 21 22:28:10 2017 Analyzing data ...\n",
      "Tue Feb 21 22:28:17 2017 Loading data for 1991/3\n",
      "Tue Feb 21 22:31:34 2017 Analyzing data ...\n",
      "Tue Feb 21 22:31:44 2017 Loading data for 1991/4\n",
      "Tue Feb 21 22:35:35 2017 Analyzing data ...\n",
      "Tue Feb 21 22:35:44 2017 Loading data for 1991/5\n",
      "Tue Feb 21 22:36:27 2017 Analyzing data ...\n",
      "Tue Feb 21 22:36:33 2017 Loading data for 1991/6\n",
      "Tue Feb 21 22:37:16 2017 Analyzing data ...\n",
      "Tue Feb 21 22:37:21 2017 Loading data for 1991/7\n",
      "Tue Feb 21 22:38:28 2017 Analyzing data ...\n",
      "Tue Feb 21 22:38:37 2017 Loading data for 1991/8\n",
      "Tue Feb 21 22:39:32 2017 Analyzing data ...\n",
      "Tue Feb 21 22:39:38 2017 Loading data for 1991/9\n",
      "Tue Feb 21 22:40:36 2017 Analyzing data ...\n",
      "Tue Feb 21 22:40:43 2017 Loading data for 1991/10\n",
      "Tue Feb 21 22:41:22 2017 Analyzing data ...\n",
      "Tue Feb 21 22:41:28 2017 Loading data for 1991/11\n",
      "Tue Feb 21 22:42:01 2017 Analyzing data ...\n",
      "Tue Feb 21 22:42:07 2017 Loading data for 1991/12\n",
      "Tue Feb 21 22:42:53 2017 Analyzing data ...\n",
      "Tue Feb 21 22:43:00 2017 Loading data for 1992/1\n",
      "Tue Feb 21 22:43:49 2017 Analyzing data ...\n",
      "Tue Feb 21 22:43:57 2017 Loading data for 1992/2\n",
      "Tue Feb 21 22:44:35 2017 Analyzing data ...\n",
      "Tue Feb 21 22:44:40 2017 Loading data for 1992/3\n",
      "Tue Feb 21 22:45:28 2017 Analyzing data ...\n",
      "Tue Feb 21 22:45:36 2017 Loading data for 1992/4\n",
      "Tue Feb 21 22:46:17 2017 Analyzing data ...\n",
      "Tue Feb 21 22:46:23 2017 Loading data for 1992/5\n",
      "Tue Feb 21 22:47:08 2017 Analyzing data ...\n",
      "Tue Feb 21 22:47:14 2017 Loading data for 1992/6\n",
      "Tue Feb 21 22:47:48 2017 Analyzing data ...\n",
      "Tue Feb 21 22:47:54 2017 Loading data for 1992/7\n",
      "Tue Feb 21 22:48:25 2017 Analyzing data ...\n",
      "Tue Feb 21 22:48:30 2017 Loading data for 1992/8\n",
      "Tue Feb 21 22:49:07 2017 Analyzing data ...\n",
      "Tue Feb 21 22:49:12 2017 Loading data for 1992/9\n",
      "Tue Feb 21 22:49:43 2017 Analyzing data ...\n",
      "Tue Feb 21 22:49:48 2017 Loading data for 1992/10\n",
      "Tue Feb 21 22:50:23 2017 Analyzing data ...\n",
      "Tue Feb 21 22:50:29 2017 Loading data for 1992/11\n",
      "Tue Feb 21 22:51:08 2017 Analyzing data ...\n",
      "Tue Feb 21 22:51:13 2017 Loading data for 1992/12\n",
      "Tue Feb 21 22:51:50 2017 Analyzing data ...\n",
      "Tue Feb 21 22:51:56 2017 Loading data for 1993/1\n",
      "Tue Feb 21 22:52:35 2017 Analyzing data ...\n",
      "Tue Feb 21 22:52:41 2017 Loading data for 1993/2\n",
      "Tue Feb 21 22:53:21 2017 Analyzing data ...\n",
      "Tue Feb 21 22:53:26 2017 Loading data for 1993/3\n",
      "Tue Feb 21 22:53:58 2017 Analyzing data ...\n",
      "Tue Feb 21 22:54:03 2017 Loading data for 1993/4\n",
      "Tue Feb 21 22:54:39 2017 Analyzing data ...\n",
      "Tue Feb 21 22:54:45 2017 Loading data for 1993/5\n",
      "Tue Feb 21 22:55:21 2017 Analyzing data ...\n",
      "Tue Feb 21 22:55:26 2017 Loading data for 1993/6\n",
      "Tue Feb 21 22:56:00 2017 Analyzing data ...\n",
      "Tue Feb 21 22:56:06 2017 Loading data for 1993/7\n",
      "Tue Feb 21 22:56:45 2017 Analyzing data ...\n",
      "Tue Feb 21 22:56:50 2017 Loading data for 1993/8\n",
      "Tue Feb 21 22:57:25 2017 Analyzing data ...\n",
      "Tue Feb 21 22:57:30 2017 Loading data for 1993/9\n",
      "Tue Feb 21 22:58:08 2017 Analyzing data ...\n",
      "Tue Feb 21 22:58:13 2017 Loading data for 1993/10\n",
      "Tue Feb 21 22:58:46 2017 Analyzing data ...\n",
      "Tue Feb 21 22:58:52 2017 Loading data for 1993/11\n",
      "Tue Feb 21 22:59:32 2017 Analyzing data ...\n",
      "Tue Feb 21 22:59:37 2017 Loading data for 1993/12\n",
      "Tue Feb 21 23:00:08 2017 Analyzing data ...\n",
      "Tue Feb 21 23:00:12 2017 Loading data for 1994/1\n",
      "Tue Feb 21 23:00:48 2017 Analyzing data ...\n",
      "Tue Feb 21 23:00:53 2017 Loading data for 1994/2\n",
      "Tue Feb 21 23:01:23 2017 Analyzing data ...\n",
      "Tue Feb 21 23:01:28 2017 Loading data for 1994/3\n",
      "Tue Feb 21 23:02:00 2017 Analyzing data ...\n",
      "Tue Feb 21 23:02:06 2017 Loading data for 1994/4\n",
      "Tue Feb 21 23:02:47 2017 Analyzing data ...\n",
      "Tue Feb 21 23:02:53 2017 Loading data for 1994/5\n",
      "Tue Feb 21 23:03:30 2017 Analyzing data ...\n",
      "Tue Feb 21 23:03:36 2017 Loading data for 1994/6\n",
      "Tue Feb 21 23:04:17 2017 Analyzing data ...\n",
      "Tue Feb 21 23:04:23 2017 Loading data for 1994/7\n",
      "Tue Feb 21 23:04:58 2017 Analyzing data ...\n",
      "Tue Feb 21 23:05:03 2017 Loading data for 1994/8\n",
      "Tue Feb 21 23:05:42 2017 Analyzing data ...\n",
      "Tue Feb 21 23:05:47 2017 Loading data for 1994/9\n",
      "Tue Feb 21 23:06:31 2017 Analyzing data ...\n",
      "Tue Feb 21 23:06:36 2017 Loading data for 1994/10\n",
      "Tue Feb 21 23:07:20 2017 Analyzing data ...\n",
      "Tue Feb 21 23:07:27 2017 Loading data for 1994/11\n",
      "Tue Feb 21 23:08:10 2017 Analyzing data ...\n",
      "Tue Feb 21 23:08:16 2017 Loading data for 1994/12\n",
      "Tue Feb 21 23:08:46 2017 Analyzing data ...\n",
      "Tue Feb 21 23:08:51 2017 Loading data for 1995/1\n",
      "Tue Feb 21 23:09:27 2017 Analyzing data ...\n",
      "Tue Feb 21 23:09:33 2017 Loading data for 1995/2\n",
      "Tue Feb 21 23:10:04 2017 Analyzing data ...\n",
      "Tue Feb 21 23:10:08 2017 Loading data for 1995/3\n",
      "Tue Feb 21 23:10:45 2017 Analyzing data ...\n",
      "Tue Feb 21 23:10:51 2017 Loading data for 1995/4\n",
      "Tue Feb 21 23:11:37 2017 Analyzing data ...\n",
      "Tue Feb 21 23:11:43 2017 Loading data for 1995/5\n",
      "Tue Feb 21 23:12:27 2017 Analyzing data ...\n",
      "Tue Feb 21 23:12:33 2017 Loading data for 1995/6\n",
      "Tue Feb 21 23:13:08 2017 Analyzing data ...\n",
      "Tue Feb 21 23:13:14 2017 Loading data for 1995/7\n",
      "Tue Feb 21 23:13:45 2017 Analyzing data ...\n",
      "Tue Feb 21 23:13:51 2017 Loading data for 1995/8\n",
      "Tue Feb 21 23:14:30 2017 Analyzing data ...\n",
      "Tue Feb 21 23:14:35 2017 Loading data for 1995/9\n",
      "Tue Feb 21 23:15:06 2017 Analyzing data ...\n",
      "Tue Feb 21 23:15:11 2017 Loading data for 1995/10\n",
      "Tue Feb 21 23:17:18 2017 Analyzing data ...\n",
      "Tue Feb 21 23:17:40 2017 Loading data for 1995/11\n",
      "Tue Feb 21 23:18:43 2017 Analyzing data ...\n",
      "Tue Feb 21 23:18:49 2017 Loading data for 1995/12\n",
      "Tue Feb 21 23:19:42 2017 Analyzing data ...\n",
      "Tue Feb 21 23:19:52 2017 Loading data for 1996/1\n",
      "Tue Feb 21 23:21:10 2017 Analyzing data ...\n",
      "Tue Feb 21 23:21:19 2017 Loading data for 1996/2\n",
      "Tue Feb 21 23:22:08 2017 Analyzing data ...\n",
      "Tue Feb 21 23:22:14 2017 Loading data for 1996/3\n",
      "Tue Feb 21 23:22:58 2017 Analyzing data ...\n",
      "Tue Feb 21 23:23:04 2017 Loading data for 1996/4\n",
      "Tue Feb 21 23:23:54 2017 Analyzing data ...\n",
      "Tue Feb 21 23:24:00 2017 Loading data for 1996/5\n",
      "Tue Feb 21 23:24:44 2017 Analyzing data ...\n",
      "Tue Feb 21 23:24:51 2017 Loading data for 1996/6\n",
      "Tue Feb 21 23:25:47 2017 Analyzing data ...\n",
      "Tue Feb 21 23:25:55 2017 Loading data for 1996/7\n",
      "Tue Feb 21 23:26:56 2017 Analyzing data ...\n",
      "Tue Feb 21 23:27:05 2017 Loading data for 1996/8\n",
      "Tue Feb 21 23:27:57 2017 Analyzing data ...\n",
      "Tue Feb 21 23:28:05 2017 Loading data for 1996/9\n",
      "Tue Feb 21 23:28:51 2017 Analyzing data ...\n",
      "Tue Feb 21 23:28:58 2017 Loading data for 1996/10\n",
      "Tue Feb 21 23:30:08 2017 Analyzing data ...\n",
      "Tue Feb 21 23:30:16 2017 Loading data for 1996/11\n",
      "Tue Feb 21 23:31:01 2017 Analyzing data ...\n",
      "Tue Feb 21 23:31:08 2017 Loading data for 1996/12\n",
      "Tue Feb 21 23:32:01 2017 Analyzing data ...\n",
      "Tue Feb 21 23:32:10 2017 Loading data for 1997/1\n",
      "Tue Feb 21 23:33:11 2017 Analyzing data ...\n",
      "Tue Feb 21 23:33:20 2017 Loading data for 1997/2\n",
      "Tue Feb 21 23:34:05 2017 Analyzing data ...\n",
      "Tue Feb 21 23:34:11 2017 Loading data for 1997/3\n",
      "Tue Feb 21 23:35:01 2017 Analyzing data ...\n",
      "Tue Feb 21 23:35:09 2017 Loading data for 1997/4\n",
      "Tue Feb 21 23:36:08 2017 Analyzing data ...\n",
      "Tue Feb 21 23:36:17 2017 Loading data for 1997/5\n",
      "Tue Feb 21 23:37:10 2017 Analyzing data ...\n",
      "Tue Feb 21 23:37:16 2017 Loading data for 1997/6\n",
      "Tue Feb 21 23:38:03 2017 Analyzing data ...\n",
      "Tue Feb 21 23:38:11 2017 Loading data for 1997/7\n",
      "Tue Feb 21 23:39:19 2017 Analyzing data ...\n",
      "Tue Feb 21 23:39:28 2017 Loading data for 1997/8\n",
      "Tue Feb 21 23:40:30 2017 Analyzing data ...\n",
      "Tue Feb 21 23:40:41 2017 Loading data for 1997/9\n",
      "Tue Feb 21 23:41:45 2017 Analyzing data ...\n",
      "Tue Feb 21 23:41:55 2017 Loading data for 1997/10\n",
      "Tue Feb 21 23:43:14 2017 Analyzing data ...\n",
      "Tue Feb 21 23:43:29 2017 Loading data for 1997/11\n",
      "Tue Feb 21 23:45:05 2017 Analyzing data ...\n",
      "Tue Feb 21 23:45:19 2017 Loading data for 1997/12\n",
      "Tue Feb 21 23:47:29 2017 Analyzing data ...\n",
      "Tue Feb 21 23:47:45 2017 Loading data for 1998/1\n",
      "Tue Feb 21 23:49:24 2017 Analyzing data ...\n",
      "Tue Feb 21 23:49:35 2017 Loading data for 1998/2\n",
      "Tue Feb 21 23:50:59 2017 Analyzing data ...\n",
      "Tue Feb 21 23:51:12 2017 Loading data for 1998/3\n",
      "Tue Feb 21 23:52:56 2017 Analyzing data ...\n",
      "Tue Feb 21 23:53:12 2017 Loading data for 1998/4\n",
      "Exception Occurred\n",
      "Tue Feb 21 23:53:12 2017 Loading data for 1998/5\n",
      "Exception Occurred\n",
      "Tue Feb 21 23:53:12 2017 Loading data for 1998/6\n",
      "Exception Occurred\n",
      "Tue Feb 21 23:53:12 2017 Loading data for 1998/7\n",
      "Tue Feb 21 23:54:38 2017 Analyzing data ...\n",
      "Tue Feb 21 23:54:56 2017 Loading data for 1998/8\n",
      "Exception Occurred\n",
      "Tue Feb 21 23:54:56 2017 Loading data for 1998/9\n",
      "Tue Feb 21 23:56:27 2017 Analyzing data ...\n",
      "Tue Feb 21 23:56:39 2017 Loading data for 1998/10\n",
      "Tue Feb 21 23:58:17 2017 Analyzing data ...\n",
      "Tue Feb 21 23:58:36 2017 Loading data for 1998/11\n",
      "Wed Feb 22 00:00:43 2017 Analyzing data ...\n",
      "Wed Feb 22 00:00:59 2017 Loading data for 1998/12\n",
      "Exception Occurred\n",
      "Wed Feb 22 00:00:59 2017 Loading data for 1999/1\n",
      "Wed Feb 22 00:02:25 2017 Analyzing data ...\n",
      "Wed Feb 22 00:02:40 2017 Loading data for 1999/2\n",
      "Wed Feb 22 00:04:10 2017 Analyzing data ...\n",
      "Wed Feb 22 00:04:21 2017 Loading data for 1999/3\n",
      "Wed Feb 22 00:05:46 2017 Analyzing data ...\n",
      "Wed Feb 22 00:06:01 2017 Loading data for 1999/4\n",
      "Wed Feb 22 00:18:22 2017 Analyzing data ...\n",
      "Wed Feb 22 00:18:38 2017 Loading data for 1999/5\n",
      "Wed Feb 22 00:27:29 2017 Analyzing data ...\n",
      "Wed Feb 22 00:27:46 2017 Loading data for 1999/6\n",
      "Wed Feb 22 00:29:25 2017 Analyzing data ...\n",
      "Wed Feb 22 00:29:39 2017 Loading data for 1999/7\n",
      "Wed Feb 22 00:31:45 2017 Analyzing data ...\n",
      "Wed Feb 22 00:32:01 2017 Loading data for 1999/8\n",
      "Wed Feb 22 00:33:59 2017 Analyzing data ...\n",
      "Wed Feb 22 00:34:17 2017 Loading data for 1999/9\n",
      "Wed Feb 22 00:35:43 2017 Analyzing data ...\n",
      "Wed Feb 22 00:35:56 2017 Loading data for 1999/10\n",
      "Wed Feb 22 00:38:00 2017 Analyzing data ...\n",
      "Wed Feb 22 00:38:18 2017 Loading data for 1999/11\n",
      "Wed Feb 22 00:40:27 2017 Analyzing data ...\n",
      "Wed Feb 22 00:40:43 2017 Loading data for 1999/12\n",
      "Wed Feb 22 00:42:24 2017 Analyzing data ...\n",
      "Wed Feb 22 00:42:37 2017 Loading data for 2000/1\n",
      "Wed Feb 22 02:49:27 2017 Analyzing data ...\n",
      "Wed Feb 22 02:49:44 2017 Loading data for 2000/2\n",
      "Wed Feb 22 03:57:33 2017 Analyzing data ...\n",
      "Wed Feb 22 03:57:49 2017 Loading data for 2000/3\n",
      "Wed Feb 22 04:51:49 2017 Analyzing data ...\n",
      "Wed Feb 22 04:52:07 2017 Loading data for 2000/4\n",
      "Wed Feb 22 07:51:39 2017 Analyzing data ...\n",
      "Wed Feb 22 08:02:34 2017 Loading data for 2000/5\n",
      "Wed Feb 22 08:51:12 2017 Analyzing data ...\n",
      "Wed Feb 22 08:51:32 2017 Loading data for 2000/6\n",
      "Wed Feb 22 08:57:36 2017 Analyzing data ...\n",
      "Wed Feb 22 08:57:59 2017 Loading data for 2000/7\n",
      "Wed Feb 22 09:00:31 2017 Analyzing data ...\n",
      "Wed Feb 22 09:00:54 2017 Loading data for 2000/8\n",
      "Wed Feb 22 09:02:34 2017 Analyzing data ...\n",
      "Wed Feb 22 09:02:50 2017 Loading data for 2000/9\n",
      "Wed Feb 22 09:04:27 2017 Analyzing data ...\n",
      "Wed Feb 22 09:04:43 2017 Loading data for 2000/10\n",
      "Wed Feb 22 09:06:36 2017 Analyzing data ...\n",
      "Wed Feb 22 09:06:54 2017 Loading data for 2000/11\n",
      "Wed Feb 22 09:09:19 2017 Analyzing data ...\n",
      "Wed Feb 22 09:09:37 2017 Loading data for 2000/12\n",
      "Wed Feb 22 09:11:07 2017 Analyzing data ...\n",
      "Wed Feb 22 09:11:23 2017 Loading data for 2001/1\n",
      "Wed Feb 22 09:14:34 2017 Analyzing data ...\n",
      "Wed Feb 22 09:14:56 2017 Loading data for 2001/2\n",
      "Wed Feb 22 09:17:02 2017 Analyzing data ...\n",
      "Wed Feb 22 09:17:18 2017 Loading data for 2001/3\n",
      "Wed Feb 22 09:19:43 2017 Analyzing data ...\n",
      "Wed Feb 22 09:20:03 2017 Loading data for 2001/4\n",
      "Wed Feb 22 09:22:35 2017 Analyzing data ...\n",
      "Wed Feb 22 09:22:52 2017 Loading data for 2001/5\n",
      "Wed Feb 22 09:24:37 2017 Analyzing data ...\n",
      "Wed Feb 22 09:24:56 2017 Loading data for 2001/6\n",
      "Wed Feb 22 09:27:02 2017 Analyzing data ...\n",
      "Wed Feb 22 09:27:18 2017 Loading data for 2001/7\n",
      "Wed Feb 22 09:29:34 2017 Analyzing data ...\n",
      "Wed Feb 22 09:29:54 2017 Loading data for 2001/8\n",
      "Wed Feb 22 09:31:39 2017 Analyzing data ...\n",
      "Wed Feb 22 09:31:59 2017 Loading data for 2001/9\n",
      "Wed Feb 22 09:33:46 2017 Analyzing data ...\n",
      "Wed Feb 22 09:34:02 2017 Loading data for 2001/10\n",
      "Wed Feb 22 09:36:35 2017 Analyzing data ...\n",
      "Wed Feb 22 09:36:52 2017 Loading data for 2001/11\n",
      "Wed Feb 22 09:38:38 2017 Analyzing data ...\n",
      "Wed Feb 22 09:38:56 2017 Loading data for 2001/12\n",
      "Wed Feb 22 09:41:43 2017 Analyzing data ...\n",
      "Wed Feb 22 09:42:03 2017 Loading data for 2002/1\n",
      "Wed Feb 22 09:44:00 2017 Analyzing data ...\n",
      "Wed Feb 22 09:44:16 2017 Loading data for 2002/2\n",
      "Wed Feb 22 09:46:30 2017 Analyzing data ...\n",
      "Wed Feb 22 09:46:49 2017 Loading data for 2002/3\n",
      "Wed Feb 22 09:48:23 2017 Analyzing data ...\n",
      "Wed Feb 22 09:48:39 2017 Loading data for 2002/4\n",
      "Wed Feb 22 09:50:21 2017 Analyzing data ...\n",
      "Wed Feb 22 09:50:38 2017 Loading data for 2002/5\n",
      "Wed Feb 22 09:53:08 2017 Analyzing data ...\n",
      "Wed Feb 22 09:53:31 2017 Loading data for 2002/6\n",
      "Wed Feb 22 09:55:55 2017 Analyzing data ...\n",
      "Wed Feb 22 09:56:14 2017 Loading data for 2002/7\n",
      "Wed Feb 22 09:59:46 2017 Analyzing data ...\n",
      "Wed Feb 22 10:00:04 2017 Loading data for 2002/8\n",
      "Wed Feb 22 10:02:25 2017 Analyzing data ...\n",
      "Wed Feb 22 10:02:44 2017 Loading data for 2002/9\n",
      "Wed Feb 22 10:05:12 2017 Analyzing data ...\n",
      "Wed Feb 22 10:05:30 2017 Loading data for 2002/10\n",
      "Wed Feb 22 10:08:22 2017 Analyzing data ...\n",
      "Wed Feb 22 10:08:42 2017 Loading data for 2002/11\n",
      "Wed Feb 22 10:11:47 2017 Analyzing data ...\n",
      "Wed Feb 22 10:12:09 2017 Loading data for 2002/12\n",
      "Wed Feb 22 10:14:10 2017 Analyzing data ...\n",
      "Wed Feb 22 10:14:32 2017 Loading data for 2003/1\n",
      "Wed Feb 22 10:17:15 2017 Analyzing data ...\n",
      "Wed Feb 22 10:17:33 2017 Loading data for 2003/2\n",
      "Wed Feb 22 10:19:27 2017 Analyzing data ...\n",
      "Wed Feb 22 10:19:45 2017 Loading data for 2003/3\n",
      "Wed Feb 22 10:23:24 2017 Analyzing data ...\n",
      "Wed Feb 22 10:23:48 2017 Loading data for 2003/4\n",
      "Wed Feb 22 10:26:14 2017 Analyzing data ...\n",
      "Wed Feb 22 10:26:35 2017 Loading data for 2003/5\n",
      "Wed Feb 22 10:29:15 2017 Analyzing data ...\n",
      "Wed Feb 22 10:29:35 2017 Loading data for 2003/6\n",
      "Wed Feb 22 10:32:08 2017 Analyzing data ...\n",
      "Wed Feb 22 10:32:33 2017 Loading data for 2003/7\n",
      "Wed Feb 22 10:35:06 2017 Analyzing data ...\n",
      "Wed Feb 22 10:35:24 2017 Loading data for 2003/8\n",
      "Wed Feb 22 10:37:36 2017 Analyzing data ...\n",
      "Wed Feb 22 10:38:03 2017 Loading data for 2003/9\n",
      "Wed Feb 22 10:41:20 2017 Analyzing data ...\n",
      "Wed Feb 22 10:41:39 2017 Loading data for 2003/10\n",
      "Wed Feb 22 10:43:32 2017 Analyzing data ...\n",
      "Wed Feb 22 10:43:52 2017 Loading data for 2003/11\n",
      "Wed Feb 22 10:48:39 2017 Analyzing data ...\n",
      "Wed Feb 22 10:49:43 2017 Loading data for 2003/12\n",
      "Wed Feb 22 10:58:03 2017 Analyzing data ...\n",
      "Wed Feb 22 10:58:58 2017 Loading data for 2004/1\n",
      "Wed Feb 22 11:04:42 2017 Analyzing data ...\n",
      "Wed Feb 22 11:05:07 2017 Loading data for 2004/2\n",
      "Wed Feb 22 11:06:57 2017 Analyzing data ...\n",
      "Wed Feb 22 11:07:15 2017 Loading data for 2004/3\n",
      "Wed Feb 22 11:09:10 2017 Analyzing data ...\n",
      "Wed Feb 22 11:09:31 2017 Loading data for 2004/4\n",
      "Wed Feb 22 11:12:27 2017 Analyzing data ...\n",
      "Wed Feb 22 11:12:47 2017 Loading data for 2004/5\n",
      "Wed Feb 22 11:14:41 2017 Analyzing data ...\n",
      "Wed Feb 22 11:14:59 2017 Loading data for 2004/6\n",
      "Wed Feb 22 11:16:55 2017 Analyzing data ...\n",
      "Wed Feb 22 11:17:11 2017 Loading data for 2004/7\n",
      "Wed Feb 22 11:20:00 2017 Analyzing data ...\n",
      "Wed Feb 22 11:20:18 2017 Loading data for 2004/8\n",
      "Wed Feb 22 11:22:37 2017 Analyzing data ...\n",
      "Wed Feb 22 11:22:54 2017 Loading data for 2004/9\n",
      "Wed Feb 22 11:25:55 2017 Analyzing data ...\n",
      "Wed Feb 22 11:26:23 2017 Loading data for 2004/10\n",
      "Wed Feb 22 11:29:03 2017 Analyzing data ...\n",
      "Wed Feb 22 11:29:25 2017 Loading data for 2004/11\n",
      "Wed Feb 22 11:31:25 2017 Analyzing data ...\n",
      "Wed Feb 22 11:31:42 2017 Loading data for 2004/12\n",
      "Wed Feb 22 11:35:31 2017 Analyzing data ...\n",
      "Wed Feb 22 11:35:50 2017 Loading data for 2005/1\n",
      "Wed Feb 22 11:37:58 2017 Analyzing data ...\n",
      "Wed Feb 22 11:38:18 2017 Loading data for 2005/2\n",
      "Wed Feb 22 11:40:26 2017 Analyzing data ...\n",
      "Wed Feb 22 11:40:41 2017 Loading data for 2005/3\n",
      "Wed Feb 22 11:42:35 2017 Analyzing data ...\n",
      "Wed Feb 22 11:42:51 2017 Loading data for 2005/4\n",
      "Wed Feb 22 11:47:20 2017 Analyzing data ...\n",
      "Wed Feb 22 11:47:46 2017 Loading data for 2005/5\n",
      "Wed Feb 22 11:49:45 2017 Analyzing data ...\n",
      "Wed Feb 22 11:50:04 2017 Loading data for 2005/6\n",
      "Wed Feb 22 11:52:08 2017 Analyzing data ...\n",
      "Wed Feb 22 11:52:24 2017 Loading data for 2005/7\n",
      "Wed Feb 22 11:54:57 2017 Analyzing data ...\n",
      "Wed Feb 22 11:55:17 2017 Loading data for 2005/8\n",
      "Wed Feb 22 11:57:46 2017 Analyzing data ...\n",
      "Wed Feb 22 11:58:04 2017 Loading data for 2005/9\n",
      "Wed Feb 22 11:59:58 2017 Analyzing data ...\n",
      "Wed Feb 22 12:00:16 2017 Loading data for 2005/10\n",
      "Wed Feb 22 12:02:04 2017 Analyzing data ...\n",
      "Wed Feb 22 12:02:21 2017 Loading data for 2005/11\n",
      "Wed Feb 22 12:04:33 2017 Analyzing data ...\n",
      "Wed Feb 22 12:04:51 2017 Loading data for 2005/12\n",
      "Wed Feb 22 12:07:16 2017 Analyzing data ...\n"
     ]
    }
   ],
   "source": [
    "for year in range(1987, 2006):\n",
    "    for month in range(1, 13):\n",
    "        print time.ctime(), \"Loading data for {}/{}\".format(year, month)\n",
    "        try:\n",
    "            nyt_data = load_nyt_data(year, month)\n",
    "        except:\n",
    "            print \"Exception Occurred\"\n",
    "            continue\n",
    "        print time.ctime(), \"Analyzing data ...\"\n",
    "        get_mentions_quotes(nyt_data, 'nyt_data_counts_enh0221_{}_{}.tsv'.format(year, month))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://techcrunch.com/2016/01/31/the-league-vs-stanford/\n",
      "{u'Amanda Bradford': (5, (u'FEMALE', 'COREF'))}\n",
      "{u'Amanda Bradford': 3}\n",
      "{u'Amanda Bradford': [(u'decided', u'decide'), (u're-posted', u're-post'), (u'wrote', u'write'), (u'said', u'say')]}\n",
      "https://techcrunch.com/2016/01/20/with-brave-software-javascripts-inventor-is-building-a-browser-for-the-ad-blocked-future/\n",
      "{u'Brendan Eich': (8, (u'MALE', 'COREF'))}\n",
      "{u'Brendan Eich': 40}\n",
      "{u'Brendan Eich': [(u'working', u'work'), (u'wrote', u'write'), (u'told', u'tell'), (u'put', u'put'), (u'said', u'say'), (u'explained', u'explain'), (u'hoping', u'hope')]}\n",
      "https://techcrunch.com/2016/01/06/stereolabss-depth-sensing-camera-helps-robots-drones-and-cars-see/\n",
      "{u'Cecile Schmollgruber': (1, (u'FEMALE', 'COREF')), u'Stereolabs': (1, (None, None)), u'Stereolabsthen': (3, (None, None))}\n",
      "{u'Cecile Schmollgruber': 0, u'Stereolabs': 0, u'Stereolabsthen': 0}\n",
      "{u'Cecile Schmollgruber': [], u'Stereolabs': [(u'says', u'say'), (u'working', u'work')], u'Stereolabsthen': [(u'uses', u'use')]}\n",
      "https://techcrunch.com/2016/01/07/no-more-ballparks-oculus-palmer-luckey-admits-screwing-up-rift-price-point-messaging/\n",
      "{u'Tom Kerriganwrote': (8, ('male', 'NAME_ONLY')), u'Nate Mitchell': (1, (None, None)), u'Palmer Luckey': (1, (u'MALE', 'COREF'))}\n",
      "{u'Tom Kerriganwrote': 0, u'Nate Mitchell': 0, u'Palmer Luckey': 0}\n",
      "{u'Tom Kerriganwrote': [], u'Nate Mitchell': [(u'told', u'tell')], u'Palmer Luckey': []}\n",
      "https://techcrunch.com/2016/01/09/how-startups-are-solving-a-decades-old-problem-in-education/\n",
      "{u'Roshan Choxi': (1, ('male', 'NAME_ONLY')), u'Socrates': (1, (None, None)), u'Udemy': (1, (None, None)), u'Udacity': (1, (None, None)), u'Thinkful': (1, (None, None)), u'Benjamin Bloom': (2, (u'MALE', 'COREF'))}\n",
      "{u'Roshan Choxi': 0, u'Socrates': 0, u'Udemy': 0, u'Udacity': 1, u'Thinkful': 0, u'Benjamin Bloom': 0}\n",
      "{u'Roshan Choxi': [], u'Udacity': [(u'added', u'add')], u'Benjamin Bloom': [(u'concludes', u'conclude'), (u'writes', u'write'), (u'imagined', u'imagine'), (u'lectures', u'lecture')], u'Socrates': [], u'Thinkful': [(u'created', u'create')], u'Udemy': [(u'partnered', u'partner')]}\n",
      "https://techcrunch.com/2016/01/12/get-your-crunchies-tickets-now-and-join-us-in-remembering-the-crunchies-of-yore/\n",
      "{u'Gowalla': (1, (None, None)), u'Zuck': (1, (None, None))}\n",
      "{u'Gowalla': 0, u'Zuck': 0}\n",
      "{u'Gowalla': [], u'Zuck': []}\n",
      "https://techcrunch.com/2016/01/06/assist-bot/\n",
      "{u'Dennis Phelps': (1, ('male', 'NAME_ONLY')), u'Dave Morin': (1, ('male', 'NAME_ONLY')), u'Jonathan Teo': (1, ('male', 'NAME_ONLY')), u'Kayvon Beykpour': (1, (None, None)), u'Robert Stephens': (1, ('male', 'NAME_ONLY')), u'Owen Van Natt': (1, ('male', 'NAME_ONLY')), u'Shane Mac': (1, ('male', 'NAME_ONLY')), u'Justin Caldbeck': (1, ('male', 'NAME_ONLY'))}\n",
      "{u'Dennis Phelps': 0, u'Dave Morin': 0, u'Jonathan Teo': 0, u'Kayvon Beykpour': 0, u'Robert Stephens': 0, u'Owen Van Natt': 0, u'Shane Mac': 10, u'Justin Caldbeck': 0}\n",
      "{u'Dennis Phelps': [], u'Dave Morin': [], u'Jonathan Teo': [], u'Kayvon Beykpour': [], u'Robert Stephens': [(u'started', u'start')], u'Owen Van Natt': [], u'Shane Mac': [(u'tells', u'tell'), (u'refers', u'refer'), (u'wants', u'want'), (u'generate', u'generate'), (u'says', u'say')], u'Justin Caldbeck': []}\n",
      "https://techcrunch.com/2016/01/08/report-nfl-will-live-stream-all-london-games-bidders-include-apple-google/\n",
      "{u'Roku': (1, (None, None))}\n",
      "{u'Roku': 0}\n",
      "{u'Roku': []}\n",
      "https://techcrunch.com/2016/01/09/endless-launch/\n",
      "{u'ProductMatt Dalio': (1, (u'MALE', 'COREF')), u'MarceloSampaio': (4, (None, None)), u'Tony Robbins': (2, ('male', 'NAME_ONLY')), u'Sampaio': (2, (None, None)), u'Nicholas Negroponte': (1, ('male', 'NAME_ONLY'))}\n",
      "{u'ProductMatt Dalio': 13, u'MarceloSampaio': 0, u'Tony Robbins': 0, u'Sampaio': 7, u'Nicholas Negroponte': 0}\n",
      "{u'ProductMatt Dalio': [(u'said', u'say'), (u'said', u'say'), (u'suggested', u'suggest'), (u'said', u'say')], u'MarceloSampaio': [], u'Tony Robbins': [], u'Nicholas Negroponte': [], u'Sampaio': [(u'described', u'describe')]}\n",
      "https://techcrunch.com/2016/01/05/tinder-push-notification-we-promise-youll-get-a-match/\n",
      "{u'Sean Rad': (4, ('male', 'NAME_ONLY'))}\n",
      "{u'Sean Rad': 0}\n",
      "{u'Sean Rad': []}\n",
      "https://techcrunch.com/2016/01/19/netflix-added-5-6-million-new-subscribers-in-the-fourth-quarter/\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "MIN_NUM = 31\n",
    "MAX_NUM = MIN_NUM + 10\n",
    "curr_idx = 0\n",
    "for link, values in tc_data.iteritems():\n",
    "    curr_idx += 1\n",
    "    if curr_idx < MIN_NUM:\n",
    "        continue\n",
    "    if curr_idx > MAX_NUM:\n",
    "        break\n",
    "\n",
    "    # if link != 'https://techcrunch.com/2016/01/05/intel-says-button-sized-curie-will-ship-in-q1-costing-under-10/':\n",
    "    #    continue\n",
    "    data = values['data']\n",
    "    corenlp = values['corenlp']\n",
    "    print link\n",
    "    #pprint(corenlp['sentences'][0]['collapsed-ccprocessed-dependencies'])\n",
    "    #print corenlp['sentences'][0]['tokens'][1:3]\n",
    "    corefs = corenlp['corefs']\n",
    "    sentences = corenlp['sentences']\n",
    "    # pprint(corefs)\n",
    "    # pprint(sentences[5])\n",
    "    # print sentences[0].keys()\n",
    "    # pprint(sentences[5]['tokens'])\n",
    "    pm = get_people_mentioned(sentences, corefs, include_gender=True)\n",
    "    quotes = get_quotes(pm, sentences, corefs)\n",
    "    verbs = get_associated_verbs(pm, sentences, corefs)\n",
    "    print pm\n",
    "    print {k: len(v) for k, v in quotes.iteritems()}\n",
    "    print verbs\n",
    "    #for c_id, coref in corefs.iteritems():\n",
    "    #    process_coref_chain(c_id, coref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nlp.utils import annotate_corenlp\n",
    "text = u'''\n",
    " President Obama said on Thursday that the United States would retaliate for Russia’s efforts to influence the presidential election, asserting that “we need to take action,” and “we will.”\n",
    "\n",
    "The comments, in an interview with NPR, indicate that Mr. Obama, in his remaining weeks in office, will pursue either economic sanctions against Russia or perhaps some kind of response in cyberspace.\n",
    "\n",
    "Mr. Obama spoke as President-elect Donald J. Trump on Thursday again refused to accept Moscow’s culpability, asking on Twitter why the administration had waited “so long to act” if Russia “or some other entity” had carried out cyberattacks.\n",
    "\n",
    "The president discussed the potential for American retaliation with Steve Inskeep of NPR for an interview to air on Friday morning. “I think there is no doubt that when any foreign government tries to impact the integrity of our election,” Mr. Obama said, “we need to take action. And we will — at the time and place of our choosing.”\n",
    "\n",
    "On Friday morning, the Kremlin’s spokesman, Dmitri S. Peskov, batted away the warning. “It is necessary to either stop talking about it, or finally produce some evidence,” he told the Interfax news agency. “Otherwise, it all begins to look quite unseemly.”\n",
    "\n",
    "The White House strongly suggested before the election that Mr. Obama would make use of sanctions authority for cyberattacks that he had given to himself by executive order. But he did not, in part out of concern that action before the election could lead to an escalated conflict.\n",
    "\n",
    "If Mr. Obama invokes sanctions on Russian individuals or organizations, Mr. Trump could reverse them. But that would be politically difficult, as his critics argue that he is blind to Russian behavior.\n",
    "\n",
    "On Thursday, pressure grew on Mr. Trump in Congress for him to acknowledge intelligence agencies’ conclusions that Russia was behind the hacking. But aides said that was all but impossible before the Electoral College convenes on Monday to formalize his victory.\n",
    "\n",
    "Mr. Trump has said privately in recent days that he believes there are people in the C.I.A. who are out to get him and are working to delegitimize his presidency, according to people briefed on the conversations who described them on the condition of anonymity.\n",
    "\n",
    "The president-elect’s suspicions have been stoked by the efforts of a group of Democratic electors, as well as one Republican, who called this week for an intelligence briefing on the Russian hacking, raising the prospect that votes in the Electoral College might be changed.\n",
    "\n",
    "In his Twitter posting on Thursday, Mr. Trump suggested that the government’s conclusions on Russian hacking were a case of sour grapes by Mr. Obama. The president-elect falsely stated that Mr. Obama had waited until after the election to raise the issue.\n",
    "\n",
    "“Why did they only complain after Hillary lost?” Mr. Trump asked, although the director of national intelligence, James R. Clapper Jr., formally blamed Russia on Oct. 7 for cyberattacks on the Democratic National Committee and other organizations.\n",
    "\n",
    "In September, meeting privately in China with President Vladimir V. Putin of Russia, Mr. Obama not only complained, the White House says, but also warned him of consequences if the Russian activity did not stop.\n",
    "\n",
    "Among those in his own party, Mr. Trump’s refusal to accept the evidence that Russia was the perpetrator was raising growing concerns, with Senator Lindsey Graham of South Carolina saying he would not vote for Rex W. Tillerson, Mr. Trump’s nominee for secretary of state, unless Mr. Tillerson addressed Russia’s role during his confirmation hearings.\n",
    "\n",
    "It remains to be seen whether Mr. Trump’s stated doubts about Russia’s involvement will subside after Monday’s Electoral College vote. He and his allies have been concerned that the reports of Russian hacking have been intended to peel away votes from him, although even Democrats have not gone so far as to say the election was illegitimate.\n",
    "\n",
    "“Right now, certain elements of the media, certain elements of the intelligence community and certain politicians are really doing the work of the Russians — they’re creating this uncertainty over the election,” Representative Peter T. King, Republican of New York, told reporters on Thursday after meeting with Mr. Trump.\n",
    "\n",
    "But many other Republicans, including Senator Mitch McConnell of Kentucky, the majority leader, and Senator John McCain of Arizona, have publicly argued that the evidence leads straight to Russia. They have called for a full investigation, and Senator Dianne Feinstein, Democrat of California, who sits on the Senate Intelligence Committee, urged Mr. Obama on Thursday to complete an administration review quickly.\n",
    "\n",
    "Mr. Trump’s Twitter post was his latest move to accuse the intelligence agencies he will soon control of acting with a political agenda and to dispute the well-documented conclusion that Moscow carried out a meticulously planned series of attacks and releases of information to interfere in the presidential race.\n",
    "\n",
    "But as he repeated his doubts, Mr. Trump seized on emerging questions about the Obama administration’s response: Why did it take months after the breaches had been discovered for the administration to name Moscow publicly as the culprit? And why did Mr. Obama initially opt not to openly retaliate, through sanctions or other measures?\n",
    "\n",
    "White House officials have said that the warning to Mr. Putin at the September summit meeting in China constituted the primary American response so far. When the administration decided to go public with its conclusion a month later, it did so in a statement from the director of national intelligence and the Homeland Security secretary, not in a prominent presidential appearance.\n",
    "\n",
    "Officials said they were worried that any larger public response would have raised doubts about the election’s integrity, something Mr. Trump was already seeking to do during the campaign when he insisted the election was “rigged.”\n",
    "\n",
    "Josh Earnest, the White House press secretary, criticized Mr. Trump on Thursday for questioning whether Russia was behind the attacks, referring to Mr. Trump’s call during the campaign for Moscow to hack Hillary Clinton’s emails, a remark his team has since dismissed as a joke.\n",
    "\n",
    "“I don’t think anybody at the White House thinks it’s funny that an adversary of the United States engaged in malicious cyberactivity to destabilize our democracy — that’s not a joke,” Mr. Earnest said. “It might be time to not attack the intelligence community, but actually be supportive of a thorough, transparent, rigorous, nonpolitical investigation into what exactly happened.”\n",
    "\n",
    "While he declined to confirm news reports that Mr. Putin was personally involved in directing the cyberattacks, Mr. Earnest pointedly read part of the Oct. 7 statement that said intelligence officials believed “that only Russia’s senior-most officials could have authorized these activities.”\n",
    "\n",
    "He said that language “would lead me to conclude that based on my personal reading and not based on any knowledge that I have that may be classified or otherwise, it was pretty obvious that they were referring to the senior-most government official in Russia.”\n",
    "\n",
    "In a conference call with reporters later on Thursday, aides declined to explain Mr. Trump’s position on whether Russia had been responsible for the breaches or to describe what he would do about the issue as president. Jason Miller, a spokesman, said he would let Mr. Trump’s “tweets speak for themselves” and added that those raising questions about the hacking were refusing to come to terms with his victory. “At a certain point you’ve got to realize that the election from last month is going to stand,” Mr. Miller said.\n",
    "'''\n",
    "ann = annotate_corenlp(text, annotators=['pos', 'lemma', 'ner', 'parse', 'depparse', 'dcoref', 'quote'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'Jason Miller': (2, (u'MALE', 'COREF')), u'Josh Earnest': (3, (u'MALE', 'COREF')), u'Steve Inskeep': (1, ('male', 'NAME_ONLY')), u'Lindsey Graham': (1, ('female', 'NAME_ONLY')), u'Hillary Clinton': (2, ('female', 'NAME_ONLY')), u'John McCain': (1, ('male', 'NAME_ONLY')), u'Vladimir V. Putin': (3, (u'MALE', 'COREF')), u'Donald J. Trump': (17, (u'MALE', 'COREF')), u'Rex W. Tillerson': (2, ('male', 'NAME_ONLY')), u'Dianne Feinstein': (1, ('female', 'NAME_ONLY')), u'Mitch McConnell': (1, ('male', 'NAME_ONLY')), u'James R. Clapper Jr.': (1, ('male', 'NAME_ONLY')), u'Dmitri S. Peskov': (1, (u'MALE', 'COREF')), u'Peter T. King': (1, ('male', 'NAME_ONLY')), u'Obama': (12, (u'MALE', 'COREF'))}\n",
      "\n",
      "{u'Jason Miller': 20, u'Josh Earnest': 119, u'Steve Inskeep': 0, u'Lindsey Graham': 0, u'Hillary Clinton': 0, u'John McCain': 0, u'Vladimir V. Putin': 0, u'Donald J. Trump': 0, u'Rex W. Tillerson': 0, u'Dianne Feinstein': 0, u'Mitch McConnell': 0, u'James R. Clapper Jr.': 0, u'Dmitri S. Peskov': 26, u'Peter T. King': 0, u'Obama': 47}\n",
      "\n",
      "{u'Jason Miller': [(u'said', u'say'), (u'let', u'let'), (u'got', u'get'), (u'said', u'say')], u'Josh Earnest': [(u'criticized', u'criticize'), (u'think', u'think'), (u'said', u'say'), (u'declined', u'decline'), (u'read', u'read'), (u'said', u'say'), (u'have', u'have')], u'Steve Inskeep': [], u'Lindsey Graham': [], u'Hillary Clinton': [], u'John McCain': [], u'Vladimir V. Putin': [(u'constituted', u'constitute'), (u'involved', u'involve')], u'Donald J. Trump': [(u'refused', u'refuse'), (u'reverse', u'reverse'), (u'acknowledge', u'acknowledge'), (u'said', u'say'), (u'working', u'work'), (u'believes', u'believe'), (u'suggested', u'suggest'), (u'asked', u'ask'), (u'concerned', u'concern'), (u'control', u'control'), (u'repeated', u'repeat'), (u'seized', u'seize'), (u'seeking', u'seek'), (u'insisted', u'insist'), (u'do', u'do')], u'Rex W. Tillerson': [], u'Dianne Feinstein': [(u'urged', u'urge'), (u'sits', u'sit')], u'Mitch McConnell': [], u'James R. Clapper Jr.': [(u'blamed', u'blame')], u'Dmitri S. Peskov': [(u'batted', u'bat'), (u'told', u'tell')], u'Peter T. King': [(u'told', u'tell'), (u'argued', u'argue')], u'Obama': [(u'said', u'say'), (u'pursue', u'pursue'), (u'spoke', u'speak'), (u'think', u'think'), (u'said', u'say'), (u'make', u'make'), (u'given', u'give'), (u'lead', u'lead'), (u'invokes', u'invoke'), (u'waited', u'wait'), (u'complained', u'complain'), (u'take', u'take'), (u'discovered', u'discover'), (u'opt', u'opt')]}\n",
      "\n",
      "defaultdict(<type 'list'>, {u'Jason Miller': ['Quoted saying 20 words', 'Subject of say'], u'Josh Earnest': ['Quoted saying 119 words', 'Subject of say'], u'Peter T. King': ['Subject of tell'], u'Donald J. Trump': ['Subject of ask, suggest, say'], u'Dmitri S. Peskov': ['Quoted saying 26 words', 'Subject of tell'], u'Obama': ['Quoted saying 47 words', 'Subject of say, speak']})\n",
      "\n",
      "{u'Dianne Feinstein': [],\n",
      " u'Dmitri S. Peskov': [],\n",
      " u'Donald J. Trump': [],\n",
      " u'Hillary Clinton': [],\n",
      " u'James R. Clapper Jr.': [],\n",
      " u'Jason Miller': [],\n",
      " u'John McCain': [],\n",
      " u'Josh Earnest': [],\n",
      " u'Lindsey Graham': [],\n",
      " u'Mitch McConnell': [],\n",
      " u'Obama': [(u'blind', u'blind')],\n",
      " u'Peter T. King': [],\n",
      " u'Rex W. Tillerson': [],\n",
      " u'Steve Inskeep': [],\n",
      " u'Vladimir V. Putin': []}\n"
     ]
    }
   ],
   "source": [
    "sentences, corefs = ann['sentences'], ann['corefs']\n",
    "people_mentioned = get_people_mentioned(sentences, corefs,\n",
    "                                        include_gender=True)\n",
    "quotes = get_quotes(people_mentioned, sentences, corefs)\n",
    "verbs = get_associated_verbs(people_mentioned, sentences, corefs)\n",
    "print people_mentioned\n",
    "print\n",
    "print {k: len(v) for k, v in quotes.iteritems()}\n",
    "print\n",
    "print verbs\n",
    "print\n",
    "print identify_sources(people_mentioned, people_to_quotes=quotes, people_to_verbs=verbs)\n",
    "print\n",
    "pprint(get_associated_adjectives(people_mentioned, sentences, corefs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
