{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "from analysis.gender import gender, gender_special\n",
    "from analysis.utils import get_people_mentioned, get_gender, get_sources\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_tc_data(year, month, folder='annotated/'):\n",
    "    tc_data = {}\n",
    "    with open('{}techcrunch_annotated_{}_{}.tsv'.format(folder, year, month), 'r') as tc_f:\n",
    "        for line in tc_f:\n",
    "            link, data, corenlp = line.strip().split('\\t')\n",
    "            tc_data[link] = {'data': json.loads(data), 'corenlp': json.loads(corenlp)}\n",
    "    \n",
    "    return tc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_mentions_counts(data):\n",
    "    all_counts = {'MALE': {'MALE': 0, 'FEMALE': 0}, 'FEMALE': {'MALE': 0, 'FEMALE': 0}, 'UNK': {'MALE': 0, 'FEMALE': 0}}\n",
    "    for link, data in data.iteritems():\n",
    "        try:\n",
    "            gend = get_gender(data['author'])\n",
    "            if type(gend) is str:\n",
    "                gend = gend.upper()\n",
    "            else:\n",
    "                gend = 'UNK'\n",
    "            if not gend:\n",
    "                continue\n",
    "            counts = all_counts[gend]\n",
    "            sentences = data['corenlp']['sentences']\n",
    "            corefs = data['corenlp']['corefs']\n",
    "            people_mentioned = process_sentences(sentences)\n",
    "            pm_to_gender = {pm: None for pm in people_mentioned}\n",
    "            for c_id, coref_chain in corefs.iteritems():\n",
    "                rep_elem = next(coref for coref in coref_chain if coref['isRepresentativeMention'])\n",
    "                elem_text = set(rep_elem['text'].split())\n",
    "                for pm in pm_to_gender:\n",
    "                    if len(set.intersection(elem_text, set(pm))) > 0:\n",
    "                        if rep_elem['gender'] in ['MALE', 'FEMALE']:\n",
    "                            pm_to_gender[pm] = rep_elem['gender']\n",
    "            for gender in ['MALE', 'FEMALE']:\n",
    "                counts[gender] += len([sp for sp in pm_to_gender.values() if sp == gender])\n",
    "        except:\n",
    "            continue\n",
    "    return all_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mentions_quotes(tc_data, out_fn):\n",
    "    for link, values in tc_data.iteritems():\n",
    "        data = values['data']\n",
    "        corenlp = values['corenlp']\n",
    "        if not type(corenlp) is dict: # This happens when CoreNLP timed out\n",
    "            continue\n",
    "        corefs = corenlp['corefs']\n",
    "        sentences = corenlp['sentences']\n",
    "        pm = get_people_mentioned(sentences, corefs, include_gender=True)\n",
    "        sources = get_sources(pm, sentences, corefs)\n",
    "        num_mentions = {'MALE': 0, 'FEMALE': 0}\n",
    "        num_distinct_mentions = {'MALE': 0, 'FEMALE': 0}\n",
    "        num_quotes = {'MALE': 0, 'FEMALE': 0}\n",
    "        for person, info in pm.iteritems():\n",
    "            count = info[0]\n",
    "            gender = info[1][0]\n",
    "            if not type(gender) is str:\n",
    "                continue\n",
    "            num_mentions[gender.upper()] += count\n",
    "            num_distinct_mentions[gender.upper()] += 1\n",
    "            quote_length = len(sources[person])\n",
    "            num_quotes[gender.upper()] += quote_length\n",
    "\n",
    "        author_gender = get_gender(data['author'])\n",
    "        if not type(author_gender) is str:\n",
    "            author_gender = 'UNKNOWN'\n",
    "        else:\n",
    "            author_gender = author_gender.upper()\n",
    "        dt = datetime.strptime(data['timestamp'], '%Y-%m-%d %H:%M:%S')\n",
    "        with open(out_fn, 'a') as out_f:\n",
    "            try:\n",
    "                out_f.write('\\t'.join([unicode(a) for a in [\n",
    "                            link, author_gender, dt.year, dt.month, ','.join(data['category']), ','.join(data['tag']), \n",
    "                            num_distinct_mentions['MALE'], num_distinct_mentions['FEMALE'],\n",
    "                            num_mentions['MALE'], num_mentions['FEMALE'],\n",
    "                            num_quotes['MALE'], num_quotes['FEMALE']\n",
    "                            ]]))\n",
    "                out_f.write('\\n')\n",
    "            except UnicodeEncodeError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tc_data = load_tc_data(2016, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_mentions_quotes(tc_data, 'tc_data_counts.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec  8 12:03:09 2016 Loading data for 2009/1\n",
      "Thu Dec  8 12:03:26 2016 Analyzing data ...\n",
      "Thu Dec  8 12:03:50 2016 Loading data for 2009/2\n",
      "Thu Dec  8 12:04:12 2016 Analyzing data ...\n",
      "Thu Dec  8 12:04:34 2016 Loading data for 2009/3\n",
      "Thu Dec  8 12:04:55 2016 Analyzing data ...\n",
      "Thu Dec  8 12:05:18 2016 Loading data for 2009/4\n",
      "Thu Dec  8 12:05:44 2016 Analyzing data ...\n",
      "Thu Dec  8 12:06:09 2016 Loading data for 2009/5\n",
      "Thu Dec  8 12:06:33 2016 Analyzing data ...\n",
      "Thu Dec  8 12:06:57 2016 Loading data for 2009/6\n",
      "Thu Dec  8 12:07:25 2016 Analyzing data ...\n",
      "Thu Dec  8 12:07:51 2016 Loading data for 2009/7\n",
      "Thu Dec  8 12:08:17 2016 Analyzing data ...\n",
      "Thu Dec  8 12:08:45 2016 Loading data for 2009/8\n",
      "Thu Dec  8 12:09:12 2016 Analyzing data ...\n",
      "Thu Dec  8 12:09:39 2016 Loading data for 2009/9\n",
      "Thu Dec  8 12:10:10 2016 Analyzing data ...\n",
      "Thu Dec  8 12:10:36 2016 Loading data for 2009/10\n",
      "Thu Dec  8 12:11:03 2016 Analyzing data ...\n",
      "Thu Dec  8 12:11:30 2016 Loading data for 2009/11\n",
      "Thu Dec  8 12:12:01 2016 Analyzing data ...\n",
      "Thu Dec  8 12:12:25 2016 Loading data for 2009/12\n",
      "Thu Dec  8 12:12:50 2016 Analyzing data ...\n",
      "Thu Dec  8 12:13:15 2016 Loading data for 2010/1\n",
      "Thu Dec  8 12:13:42 2016 Analyzing data ...\n",
      "Thu Dec  8 12:14:08 2016 Loading data for 2010/2\n",
      "Thu Dec  8 12:14:37 2016 Analyzing data ...\n",
      "Thu Dec  8 12:15:03 2016 Loading data for 2010/3\n",
      "Thu Dec  8 12:15:33 2016 Analyzing data ...\n",
      "Thu Dec  8 12:16:03 2016 Loading data for 2010/4\n",
      "Thu Dec  8 12:16:35 2016 Analyzing data ...\n",
      "Thu Dec  8 12:17:03 2016 Loading data for 2010/5\n",
      "Thu Dec  8 12:17:33 2016 Analyzing data ...\n",
      "Thu Dec  8 12:17:57 2016 Loading data for 2010/6\n",
      "Thu Dec  8 12:18:23 2016 Analyzing data ...\n",
      "Thu Dec  8 12:18:47 2016 Loading data for 2010/7\n",
      "Thu Dec  8 12:19:14 2016 Analyzing data ...\n",
      "Thu Dec  8 12:19:39 2016 Loading data for 2010/8\n",
      "Thu Dec  8 12:20:09 2016 Analyzing data ...\n",
      "Thu Dec  8 12:20:35 2016 Loading data for 2010/9\n",
      "Thu Dec  8 12:21:08 2016 Analyzing data ...\n",
      "Thu Dec  8 12:21:39 2016 Loading data for 2010/10\n",
      "Thu Dec  8 12:22:08 2016 Analyzing data ...\n",
      "Thu Dec  8 12:22:36 2016 Loading data for 2010/11\n",
      "Thu Dec  8 12:23:05 2016 Analyzing data ...\n",
      "Thu Dec  8 12:23:32 2016 Loading data for 2010/12\n",
      "Thu Dec  8 12:24:01 2016 Analyzing data ...\n",
      "Thu Dec  8 12:24:26 2016 Loading data for 2011/1\n",
      "Thu Dec  8 12:24:54 2016 Analyzing data ...\n",
      "Thu Dec  8 12:25:18 2016 Loading data for 2011/2\n",
      "Thu Dec  8 12:25:44 2016 Analyzing data ...\n",
      "Thu Dec  8 12:26:10 2016 Loading data for 2011/3\n",
      "Thu Dec  8 12:26:45 2016 Analyzing data ...\n",
      "Thu Dec  8 12:27:14 2016 Loading data for 2011/4\n",
      "Thu Dec  8 12:27:42 2016 Analyzing data ...\n",
      "Thu Dec  8 12:28:06 2016 Loading data for 2011/5\n",
      "Thu Dec  8 12:28:32 2016 Analyzing data ...\n",
      "Thu Dec  8 12:28:56 2016 Loading data for 2011/6\n",
      "Thu Dec  8 12:29:25 2016 Analyzing data ...\n",
      "Thu Dec  8 12:29:51 2016 Loading data for 2011/7\n",
      "Thu Dec  8 12:30:14 2016 Analyzing data ...\n",
      "Thu Dec  8 12:30:35 2016 Loading data for 2011/8\n",
      "Thu Dec  8 12:31:04 2016 Analyzing data ...\n",
      "Thu Dec  8 12:31:24 2016 Loading data for 2011/9\n",
      "Thu Dec  8 12:31:48 2016 Analyzing data ...\n",
      "Thu Dec  8 12:32:08 2016 Loading data for 2011/10\n",
      "Thu Dec  8 12:32:40 2016 Analyzing data ...\n",
      "Thu Dec  8 12:33:02 2016 Loading data for 2011/11\n",
      "Thu Dec  8 12:33:29 2016 Analyzing data ...\n",
      "Thu Dec  8 12:33:50 2016 Loading data for 2011/12\n",
      "Thu Dec  8 12:34:21 2016 Analyzing data ...\n",
      "Thu Dec  8 12:34:42 2016 Loading data for 2012/1\n",
      "Thu Dec  8 12:35:10 2016 Analyzing data ...\n",
      "Thu Dec  8 12:35:32 2016 Loading data for 2012/2\n",
      "Thu Dec  8 12:36:03 2016 Analyzing data ...\n",
      "Thu Dec  8 12:36:24 2016 Loading data for 2012/3\n",
      "Thu Dec  8 12:36:54 2016 Analyzing data ...\n",
      "Thu Dec  8 12:37:15 2016 Loading data for 2012/4\n",
      "Thu Dec  8 12:37:47 2016 Analyzing data ...\n",
      "Thu Dec  8 12:38:08 2016 Loading data for 2012/5\n",
      "Thu Dec  8 12:38:43 2016 Analyzing data ...\n",
      "Thu Dec  8 12:39:05 2016 Loading data for 2012/6\n",
      "Thu Dec  8 12:39:37 2016 Analyzing data ...\n",
      "Thu Dec  8 12:39:57 2016 Loading data for 2012/7\n",
      "Thu Dec  8 12:40:32 2016 Analyzing data ...\n",
      "Thu Dec  8 12:41:05 2016 Loading data for 2012/8\n",
      "Thu Dec  8 12:41:39 2016 Analyzing data ...\n",
      "Thu Dec  8 12:42:05 2016 Loading data for 2012/9\n",
      "Thu Dec  8 12:42:44 2016 Analyzing data ...\n",
      "Thu Dec  8 12:43:11 2016 Loading data for 2012/10\n",
      "Thu Dec  8 12:43:53 2016 Analyzing data ...\n",
      "Thu Dec  8 12:44:22 2016 Loading data for 2012/11\n",
      "Thu Dec  8 12:44:58 2016 Analyzing data ...\n",
      "Thu Dec  8 12:45:22 2016 Loading data for 2012/12\n",
      "Thu Dec  8 12:45:59 2016 Analyzing data ...\n",
      "Thu Dec  8 12:46:21 2016 Loading data for 2013/1\n",
      "Thu Dec  8 12:47:01 2016 Analyzing data ...\n",
      "Thu Dec  8 12:47:27 2016 Loading data for 2013/2\n",
      "Thu Dec  8 12:48:07 2016 Analyzing data ...\n",
      "Thu Dec  8 12:48:32 2016 Loading data for 2013/3\n",
      "Thu Dec  8 12:49:09 2016 Analyzing data ...\n",
      "Thu Dec  8 12:49:34 2016 Loading data for 2013/4\n",
      "Thu Dec  8 12:50:15 2016 Analyzing data ...\n",
      "Thu Dec  8 12:50:42 2016 Loading data for 2013/5\n",
      "Thu Dec  8 12:51:18 2016 Analyzing data ...\n",
      "Thu Dec  8 12:51:40 2016 Loading data for 2013/6\n",
      "Thu Dec  8 12:52:11 2016 Analyzing data ...\n",
      "Thu Dec  8 12:52:33 2016 Loading data for 2013/7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-4ba4e2dad884>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Loading data for {}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mtc_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_tc_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Analyzing data ...\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mget_mentions_quotes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtc_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tc_data_counts.tsv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-9f55cfe4b940>\u001b[0m in \u001b[0;36mload_tc_data\u001b[1;34m(year, month, folder)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtc_f\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mlink\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorenlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mtc_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'corenlp'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorenlp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtc_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python2.7/json/__init__.pyc\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 326\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python2.7/json/decoder.pyc\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \"\"\"\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python2.7/json/decoder.pyc\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \"\"\"\n\u001b[0;32m    381\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No JSON object could be decoded\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for year in range(2009, 2017):\n",
    "    for month in range(1, 13):\n",
    "        if year == 2016 and month > 7:\n",
    "            break\n",
    "        print time.ctime(), \"Loading data for {}/{}\".format(year, month)\n",
    "        tc_data = load_tc_data(year, month)\n",
    "        print time.ctime(), \"Analyzing data ...\"\n",
    "        get_mentions_quotes(tc_data, 'tc_data_counts.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://techcrunch.com/2016/01/14/nasa-receives-patent-for-a-new-type-of-squishy-amorphous-robot/\n",
      "{u'Imagecourtesy': (5, (None, None)), u'Arthur Bradley': (1, (u'MALE', 'COREF'))}\n",
      "{u'Imagecourtesy': 0, u'Arthur Bradley': 1}\n",
      "https://techcrunch.com/2016/01/20/apple-releases-music-memos-a-recorder-app-for-musicians/\n",
      "{}\n",
      "{}\n",
      "https://techcrunch.com/2016/01/16/notify-nearby-launch/\n",
      "{u'Nevin Jethmalani': (3, (u'MALE', 'COREF')), u'Levi': (1, ('male', 'NAME_ONLY'))}\n",
      "{u'Nevin Jethmalani': 84, u'Levi': 0}\n",
      "https://techcrunch.com/2016/01/27/watch-microsoft-ventures-london-accelerator-right-here-2/\n",
      "{}\n",
      "{}\n",
      "https://techcrunch.com/2016/01/18/why-big-companies-keep-failing-the-stack-fallacy/\n",
      "{u'Anshu Sharma': (1, ('male', 'NAME_ONLY')), u'Larry Ellison': (1, (u'MALE', 'COREF'))}\n",
      "{u'Anshu Sharma': 0, u'Larry Ellison': 0}\n",
      "https://techcrunch.com/2016/01/26/ipo-slowdown-a-look-at-company-profitability/\n",
      "{u'Ben Narasin': (1, ('male', 'NAME_ONLY')), u'Jeremy': (1, ('male', 'NAME_ONLY'))}\n",
      "{u'Ben Narasin': 0, u'Jeremy': 0}\n",
      "https://techcrunch.com/2016/01/05/heres-why-samsungs-new-frankenstein-fridge-is-actually-dumb/\n",
      "{}\n",
      "{}\n",
      "https://techcrunch.com/2016/01/22/gravit-lets-you-illustrate-in-your-abode-or-on-the-road/\n",
      "{u'Alexander Adam': (3, (u'MALE', 'COREF')), u'Gravit': (2, (None, None))}\n",
      "{u'Alexander Adam': 132, u'Gravit': 0}\n",
      "https://techcrunch.com/2016/01/27/apple-has-fixed-bug-that-was-crashing-safari-at-least-on-os-x/\n",
      "{u'Sean Sullivan': (1, ('male', 'NAME_ONLY'))}\n",
      "{u'Sean Sullivan': 0}\n",
      "https://techcrunch.com/2016/01/14/indian-home-healthcare-startup-care24-gets-4m-series-a-from-saif-partners/\n",
      "{u'Pranshu Sharma': (6, (None, None)), u'Garima Tripathi': (1, (None, None)), u'Vipin Pathak': (1, (u'MALE', 'COREF')), u'Abhishek Tiwari': (1, (None, None))}\n",
      "{u'Pranshu Sharma': 0, u'Garima Tripathi': 0, u'Vipin Pathak': 65, u'Abhishek Tiwari': 0}\n",
      "https://techcrunch.com/2016/01/06/is-10-too-old-to-be-in-tech/\n",
      "{}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "MIN_NUM = 11\n",
    "MAX_NUM = MIN_NUM + 10\n",
    "curr_idx = 0\n",
    "for link, values in tc_data.iteritems():\n",
    "    curr_idx += 1\n",
    "    if curr_idx < MIN_NUM:\n",
    "        continue\n",
    "    if curr_idx > MAX_NUM:\n",
    "        break\n",
    "\n",
    "    # if link != 'https://techcrunch.com/2016/01/05/intel-says-button-sized-curie-will-ship-in-q1-costing-under-10/':\n",
    "    #    continue\n",
    "    data = values['data']\n",
    "    corenlp = values['corenlp']\n",
    "    print link\n",
    "    corefs = corenlp['corefs']\n",
    "    sentences = corenlp['sentences']\n",
    "    # pprint(corefs)\n",
    "    # pprint(sentences[5])\n",
    "    # print sentences[0].keys()\n",
    "    # pprint(sentences[5]['tokens'])\n",
    "    pm = get_people_mentioned(sentences, corefs, include_gender=True)\n",
    "    sources = get_sources(pm, sentences, corefs)\n",
    "    print pm\n",
    "    print {k: len(v) for k, v in sources.iteritems()}\n",
    "    #for c_id, coref in corefs.iteritems():\n",
    "    #    process_coref_chain(c_id, coref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
