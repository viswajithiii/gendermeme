---
title: "Error Analysis: GenderMeme May 19 2017"
author: "Poorna Kumar"
date: 5-19-2017
output: 
  github_document:
    toc: true
    toc_depth: 6
---

```{r setup, include=FALSE}
# knitr options
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
library(stringr)
library(knitr)

return_outlet <- function(url) {
  if (str_detect(url, "techcrunch\\.c")) {
    return("TechCrunch")
  }
  if (str_detect(url, "washingtonpost\\.c")) {
    return("Washington Post")
  }
  if (str_detect(url, "nytimes\\.com")) {
    return("NYT")
  } 
  if (str_detect(url, "latimes\\.c")) {
    return("LA Times")
  }
  if (str_detect(url, "bloomberg\\.c")) {
    return("Bloomberg")
  }
}

error_data <- 
  read_tsv('/Users/Poorna/Desktop/Box Sync/Gendermeme/gendermeme/annotated/manual/ann_dump_Fri_May_19_00_59_45_2017.tsv')
```

## PRECISION AND RECALL FOR PERSON IDENTIFICATION

```{r}
error_data <-
  error_data %>% 
  filter(!(a_gender == "non-living" & where == "auto_only")) %>%
  filter(!(a_gender == "non-living" & str_detect(m_gender, "company"))) %>% 
  filter(!(where == "manual_only" & str_detect(m_gender, "company"))) %>% 
  filter(!(str_detect(name, "(anonymous)|(unnamed)|(unknown)"))) %>% 
  mutate(outlet = map_chr(url, return_outlet))
```

The error rates for people-detection are below:

```{r}
error_data %>% 
  group_by(outlet) %>% 
  mutate(n_articles = n_distinct(art_id)) %>% 
  count(n_articles, where) %>% 
  spread(key = where, value = n) %>% 
  ungroup() %>% 
  mutate(precision = both / (auto_only + both), recall = both / (both + manual_only)) %>% 
  arrange(desc(n_articles)) %>% 
  kable()
```

We now examine the error rates for gender detection.

## RECALL FOR GENDER

First, let us look at those cases where a person was identified by automatically
as well as manually. We restrict our attention to people who were identified as
either `male` or `female`.

```{r}
error_data %>% 
  filter(where == "both", m_gender %in% c("male", "female")) %>% 
  count(m_gender, a_gender) %>% 
  group_by(m_gender) %>% 
  summarize(
    total = sum(n), 
    a_correct = sum(n[a_gender == m_gender]), 
    a_incorrect_none = sum(n[a_gender == "None"]),
    a_incorrect_opp_gender = sum(n[m_gender != a_gender & a_gender %in% c("male", "female")]),
    a_incorrect_non_living = sum(n[m_gender != a_gender & a_gender == "non-living"])
  ) %>% 
  mutate(
    perc_a_correct = a_correct * 100 / total,
    perc_a_incorrect_none = a_incorrect_none * 100 / total,
    perc_a_incorrect_opp_gender = a_incorrect_opp_gender * 100 / total,
    perc_a_incorrect_non_living = a_incorrect_non_living * 100 / total
  ) %>% 
  kable()
```


We find that a lot of men are being mis-tagged as women. Why is this?

Let's have a look at men who are tagged as women.

```{r}
error_data %>% 
  filter(where == "both", m_gender == "male", a_gender == "female") %>% 
  select(art_id, outlet, everything(), -url) %>% 
  kable()
```

Let's also look at men who are tagged as "None":

```{r}
error_data %>% 
  filter(where == "both", m_gender == "male", a_gender == "None") %>% 
  select(art_id, outlet, everything(), -url) %>% 
  kable()

```


We find that, quite frequently, "Trump" is being tagged as male by us and as "None"
by the computer. We suspect that this is because "Trump" appears in contexts like
"Trump administration", only once in the article, etc. A rule might be able to 
fix this issue.

Let us look at the gender-related accuracy once again:

```{r}
error_data %>% 
  filter(outlet %in% c("NYT", "Washington Post", "TechCrunch")) %>% 
  filter(where == "both", m_gender %in% c("male", "female")) %>% 
  group_by(outlet) %>% 
  count(m_gender, a_gender) %>% 
  group_by(outlet, m_gender) %>% 
  summarize(
    total = sum(n), 
    a_correct = sum(n[a_gender == m_gender]), 
    a_incorrect_none = sum(n[a_gender == "None"]),
    a_incorrect_opp_gender = sum(n[m_gender != a_gender & a_gender %in% c("male", "female")]),
    a_incorrect_non_living = sum(n[m_gender != a_gender & a_gender == "non-living"])
  ) %>% 
  mutate(
    perc_a_correct = a_correct * 100 / total,
    perc_a_incorrect_none = a_incorrect_none * 100 / total,
    perc_a_incorrect_opp_gender = a_incorrect_opp_gender * 100 / total,
    perc_a_incorrect_non_living = a_incorrect_non_living * 100 / total
  ) %>% 
  ungroup() %>% 
  kable()
```

Note that this is a slightly strange, and over-optimistic, metric. We are isolating 
only those people that WERE identified as people, and among those, finding the recall
for gender. However, some male and female people fall out of the pipeline at the person-detection
stage, so the above recall numbers are slightly inflated. Let's correct for this.

```{r}
error_data %>% 
  filter(
    outlet %in% c("NYT", "TechCrunch", "Washington Post"), 
    m_gender %in% c("male", "female"), 
    (where == "manual_only" | where == "both")
  ) %>% 
  group_by(outlet) %>% 
  count(m_gender, a_gender) %>% 
  mutate(a_gender = ifelse(is.na(a_gender), "person not identified", a_gender)) %>% 
  group_by(outlet, m_gender) %>% 
  summarize(
    total = sum(n), 
    a_correct = sum(n[a_gender == m_gender]), 
    a_incorrect_none = sum(n[a_gender == "None"]),
    a_incorrect_opp_gender = sum(n[m_gender != a_gender & a_gender %in% c("male", "female")]),
    a_incorrect_non_living = sum(n[m_gender != a_gender & a_gender == "non-living"]),
    a_person_not_identified = sum(n[m_gender != a_gender & a_gender == "person not identified"])
  ) %>% 
  mutate(
    perc_a_correct = a_correct * 100 / total,
    perc_a_incorrect_none = a_incorrect_none * 100 / total,
    perc_a_incorrect_opp_gender = a_incorrect_opp_gender * 100 / total,
    perc_a_incorrect_non_living = a_incorrect_non_living * 100 / total,
    perc_a_person_not_identified = a_person_not_identified * 100 / total
  ) %>% 
  ungroup() %>% 
  kable()
```

This table gives us true recall for genders. We probably need more data for Washington Post.

## PRECISION FOR GENDER

Let's see what our precision is for genders. That is, when we say someone is male,
how often are they really male? Or when we say someone is female, how often are they 
really female? This should (hopefully) be easy to find.

```{r}
error_data %>% 
  filter(
    (where == "auto_only" | where == "both"),
    outlet %in% c("NYT", "Washington Post", "TechCrunch"),
    a_gender %in% c("male", "female")
  ) %>% 
  group_by(outlet) %>% 
  count(a_gender, m_gender) %>% 
  mutate(m_gender = ifelse(is.na(m_gender), "not a person", m_gender)) %>% 
  group_by(outlet, a_gender) %>% 
  summarize(
    total = sum(n),
    correct = sum(n[a_gender == m_gender]),
    true_gender_opposite = sum(n[m_gender != a_gender & m_gender %in% c("male", "female")]),
    not_a_person = sum(n[m_gender == "not a person"])
  ) %>% 
  mutate(
    precision = correct * 100 / total,
    true_gender_opp_perc = true_gender_opposite * 100 / total,
    not_a_person_perc = not_a_person * 100 / total
  ) %>% 
  ungroup() %>% 
  kable()

```

Precision is not as good as recall. This is a little worrying: if recall was about 
the same for each gender, but precision were really high, we'd still have a good 
tool. If recall is high, but precision is low, we're not in good shape.